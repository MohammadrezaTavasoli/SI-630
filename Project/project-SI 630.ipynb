{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSU7yERLP_66"
   },
   "source": [
    "## 1.1. Using Colab GPU for Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DEfSbAA4QHas",
    "outputId": "06c068c1-28dc-4ae7-cd62-de42504c3c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oYsV4H8fCpZ-",
    "outputId": "59063679-6302-4f9e-e043-7b0ddccef624"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ElsnSNUridI"
   },
   "source": [
    "## 1.2. Installing the Hugging Face Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "0NmMdkZO8R6q",
    "outputId": "3a2d7e0f-b6bb-4d17-bcba-95f796b338a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
      "\u001b[K     |████████████████████████████████| 573kB 9.4MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 23.8MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 55.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 56.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.46)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.46)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=bac89e20cded8d4480c8cdcf90b6e99856ebe6233f60e44cbb74ec4ef10b2a55\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "5m6AnuFv0QXQ",
    "outputId": "9105ae99-4b13-47fe-e267-9ce1cb028a0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=2de7cdcccabe1acac6bdc7e0df6b67c3e70f9f0cc6049c6f6d9b9f92eec2c0db\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08pO03Ff1BjI"
   },
   "source": [
    "The dataset is hosted on GitHub in this repo: https://nyu-mll.github.io/CoLA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pMtmPMkBzrvs",
    "outputId": "86e99536-6a33-4cc3-c363-d3549d801a13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "print('Downloading dataset...')\n",
    "\n",
    "# The URL for the dataset zip file.\n",
    "url ='https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip?attredirects=0&d=1'\n",
    "\n",
    "# Download the file (if we haven't already)\n",
    "if not os.path.exists('./olid.zip'):\n",
    "    wget.download(url, './olid.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mKctx-ll2FB"
   },
   "source": [
    "Unzip the dataset to the file system. You can browse the file system of the Colab instance in the sidebar on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "0Yv-tNv20dnH",
    "outputId": "a4f8d31b-4db0-4a0d-f6c3-b18ca359d59b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  olid.zip\n",
      "  inflating: olid-annotation.txt     \n",
      "  inflating: olid-training-v1.0.tsv  \n",
      "  inflating: README.txt              \n",
      "  inflating: labels-levela.csv       \n",
      "  inflating: labels-levelb.csv       \n",
      "  inflating: labels-levelc.csv       \n",
      "  inflating: testset-levelc.tsv      \n",
      "  inflating: testset-levelb.tsv      \n",
      "  inflating: testset-levela.tsv      \n"
     ]
    }
   ],
   "source": [
    "# Unzip the dataset (if we haven't already)\n",
    "if not os.path.exists('./olid/'):\n",
    "    !unzip olid.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQUy9Tat2EF_"
   },
   "source": [
    "## 2.2. Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYWzeGSY2xh3"
   },
   "source": [
    "We'll use pandas to parse the \"in-domain\" training set and look at a few of its properties and data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "_UkeC7SG2krJ",
    "outputId": "089e9c27-72cf-4cda-f1d5-d1f87054b67a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 13,241\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtaska</th>\n",
       "      <th>subtaskb</th>\n",
       "      <th>subtaskc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45439</th>\n",
       "      <td>@USER @USER She is. Seriously. Ive cried so ha...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74318</th>\n",
       "      <td>@USER I don’t care how good a player he is if ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17537</th>\n",
       "      <td>@USER My man! Let’s go!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40895</th>\n",
       "      <td>@USER The crap they come up with as excuses\" c...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99194</th>\n",
       "      <td>@USER @USER Liberals makes America  Sicker</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32374</th>\n",
       "      <td>@USER They're all Democrats right?   They're n...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37862</th>\n",
       "      <td>@USER @USER @USER He is being sued for sexual ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10719</th>\n",
       "      <td>@USER @USER We need a marijuana lobby that can...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41321</th>\n",
       "      <td>@USER That's awesome!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26346</th>\n",
       "      <td>@USER Never had. Let's fuck it up.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  ... subtaskc\n",
       "45439  @USER @USER She is. Seriously. Ive cried so ha...  ...      NaN\n",
       "74318  @USER I don’t care how good a player he is if ...  ...      IND\n",
       "17537                            @USER My man! Let’s go!  ...      NaN\n",
       "40895  @USER The crap they come up with as excuses\" c...  ...      NaN\n",
       "99194         @USER @USER Liberals makes America  Sicker  ...      NaN\n",
       "32374  @USER They're all Democrats right?   They're n...  ...      IND\n",
       "37862  @USER @USER @USER He is being sued for sexual ...  ...      NaN\n",
       "10719  @USER @USER We need a marijuana lobby that can...  ...      IND\n",
       "41321                              @USER That's awesome!  ...      NaN\n",
       "26346                 @USER Never had. Let's fuck it up.  ...      IND\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"./olid-training-v1.0.tsv\", delimiter='\\t', header=None, names=['tweet', 'subtaska', 'subtaskb', 'subtaskc'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "blqIvQaQncdJ",
    "outputId": "097a9e50-1e39-4482-994f-2ff4bba6f7d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtaska</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24621</th>\n",
       "      <td>@USER you are some basic white girl who clearl...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20092</th>\n",
       "      <td>12 hotel guests killed in Corpus Christi with ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37835</th>\n",
       "      <td>@USER @USER @USER @USER Kate, gun safety\" is j...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47662</th>\n",
       "      <td>@USER Guess he forgot to walk the liberal plank!</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16289</th>\n",
       "      <td>@USER @USER @USER @USER have become the party ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet subtaska\n",
       "24621  @USER you are some basic white girl who clearl...      OFF\n",
       "20092  12 hotel guests killed in Corpus Christi with ...      OFF\n",
       "37835  @USER @USER @USER @USER Kate, gun safety\" is j...      OFF\n",
       "47662   @USER Guess he forgot to walk the liberal plank!      OFF\n",
       "16289  @USER @USER @USER @USER have become the party ...      OFF"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.subtaska == 'OFF'].sample(5)[['tweet', 'subtaska']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "sentences = df.tweet.values\n",
    "labels = df.subtaska.values\n",
    "labels2=df.subtaskb.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ex5O1eV-Pfct"
   },
   "source": [
    "# 3. Tokenization & Input Formatting\n",
    "\n",
    "In this section, we'll transform our dataset into the format that BERT can be trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8kEDRvShcU5"
   },
   "source": [
    "## 3.1. BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Z474sSC6oe7A",
    "outputId": "71fff02f-ea7c-477c-96a3-d11a5af5d7d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFzmtleW6KmJ"
   },
   "source": [
    "Let's apply the tokenizer to one sentence just to see the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "dLIbudgfh6F0",
    "outputId": "9e3c79d1-c797-4bb8-e836-1d56eddbbe0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  @USER She should ask a few native Americans what their take on this is.\n",
      "Tokenized:  ['@', 'user', 'she', 'should', 'ask', 'a', 'few', 'native', 'americans', 'what', 'their', 'take', 'on', 'this', 'is', '.']\n",
      "Token IDs:  [1030, 5310, 2016, 2323, 3198, 1037, 2261, 3128, 4841, 2054, 2037, 2202, 2006, 2023, 2003, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[1])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[1]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viKGCCh8izww"
   },
   "source": [
    "## 3.2. Required Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6w8elb-58GJ"
   },
   "source": [
    "## 3.3. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cKsH2sU0OCQA",
    "outputId": "016574bd-c500-4c04-e1be-64511ba9cd29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  171\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1M296yz577fV"
   },
   "source": [
    "Just in case there are some longer test sentences, I'll set the maximum length to 64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "UiKSUxRDez0I",
    "outputId": "c47d1c84-3f65-47fd-e875-4b4cb69fa767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13241\n"
     ]
    }
   ],
   "source": [
    "label=[]\n",
    "for i in range(len(labels)):\n",
    "      if labels[i]=='OFF':\n",
    "        label.append(0)\n",
    "      else:\n",
    "        label.append(1)\n",
    "print()\n",
    "print(len(label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rJseK0l0jgy-",
    "outputId": "0fafa90c-db25-46ed-b71e-6a801f526371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13241\n"
     ]
    }
   ],
   "source": [
    "label2=[]\n",
    "for i in range(len(labels2)):\n",
    "      if labels2[i]=='TIN':\n",
    "        label2.append(1)\n",
    "      else:\n",
    "        label2.append(0)\n",
    "print()\n",
    "print(len(labels2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "2bBdb3pt8LuQ",
    "outputId": "3a74c60f-8cc2-4023-8eb5-8a7e990639cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  @USER She should ask a few native Americans what their take on this is.\n",
      "Token IDs: tensor([ 101, 1030, 5310, 2016, 2323, 3198, 1037, 2261, 3128, 4841, 2054, 2037,\n",
      "        2202, 2006, 2023, 2003, 1012,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(label)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[1])\n",
    "print('Token IDs:', input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "kINh-OKf3mzg",
    "outputId": "e38f5f53-0cda-4962-be06-a9fbd4312b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  @USER She should ask a few native Americans what their take on this is.\n",
      "Token IDs: tensor([ 101, 1030, 5310, 2016, 2323, 3198, 1037, 2261, 3128, 4841, 2054, 2037,\n",
      "        2202, 2006, 2023, 2003, 1012,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks= []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                     \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 256,           \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,  \n",
    "                        return_tensors = 'pt',    \n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels2 = torch.tensor(label2)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[1])\n",
    "print('Token IDs:', input_ids[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRp4O7D295d_"
   },
   "source": [
    "## 3.4. Training & Validation Split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qu0ao7p8rb06"
   },
   "source": [
    "Divide up our training set to use 90% for training and 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GEgLpFVlo1Z-",
    "outputId": "ace86a30-18dd-4a76-b7bb-dfd54ef722ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,916 training samples\n",
      "1,325 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3nEuotyWFZCn",
    "outputId": "181392aa-3524-4df7-ee9c-5d58d7a1db02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,916 training samples\n",
      "1,325 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset2 = TensorDataset(input_ids, attention_masks, labels2)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset2))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset2, val_dataset2 = random_split(dataset2, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dD9i6Z2pG-sN"
   },
   "source": [
    "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGUqOCtgqGhP"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lm9wU9H_FoIa"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader2 = DataLoader(\n",
    "            train_dataset2,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader2 = DataLoader(\n",
    "            val_dataset2, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bwa6Rts-02-"
   },
   "source": [
    "# 4. Train Our Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6TKgyUzPIQc"
   },
   "source": [
    "## 4.1. BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d99d4da3fd52476b97df5fa5fb56e632",
      "6f011e997a0244e9b4c411ed126c5bf2",
      "9a0a6d8a4ea14081957c1c6931e36516",
      "13f7e3eb985047edb280c039c9e9d5d0",
      "cb0602500cef422ebc91c6b61a53c242",
      "cb0be6ae80a54283a34860502f77ffc8",
      "05d6b1d7982644e5b4db88f048035ba7",
      "93efcaeb7b5040f9a5e926fff547d16e",
      "b7f400b9b8774c6ca6e08f6fa67cdbaf",
      "059082807ff446f196a83a8844aa6bc0",
      "884cc5bf6ece437dbfb8221b3f90a549",
      "2115a9df3e2744908f9e4baae71e123d",
      "49d3727caad24f10aa0cea67fe3e513f",
      "5c3b6f5b995043b7b0f9ec3bb7d271af",
      "c98c6d5271c24f7bb171527accc7a604",
      "e383abb30458425a8fc7105b4fd1fdd7"
     ]
    },
    "colab_type": "code",
    "id": "gFsCTp_mporB",
    "outputId": "7d12505b-95c9-48f7-fc8c-1bd1cab6195a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99d4da3fd52476b97df5fa5fb56e632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=433, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f400b9b8774c6ca6e08f6fa67cdbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "8PIiVlDYCtSq",
    "outputId": "0e605eca-039d-4e5a-b414-749baa6f27bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRWT-D4U_Pvx"
   },
   "source": [
    "## 4.2. Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLs72DuMODJO"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-p0upAhhRiIx"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqfmWwUR_Sox"
   },
   "source": [
    "## 4.3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cQNvaZ9bnyy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KNhRtWPXH9C3"
   },
   "source": [
    "Helper function for formatting elapsed times as `hh:mm:ss`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpt6tR83keZD"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfNIhN19te3N"
   },
   "source": [
    "We're ready to kick off the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6J-FYdx6nFE_",
    "outputId": "60d15868-7fe1-4e87-b4b2-db3ac132e1c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of  1,490.    Elapsed: 0:00:19.\n",
      "  Batch    80  of  1,490.    Elapsed: 0:00:37.\n",
      "  Batch   120  of  1,490.    Elapsed: 0:00:55.\n",
      "  Batch   160  of  1,490.    Elapsed: 0:01:14.\n",
      "  Batch   200  of  1,490.    Elapsed: 0:01:32.\n",
      "  Batch   240  of  1,490.    Elapsed: 0:01:51.\n",
      "  Batch   280  of  1,490.    Elapsed: 0:02:10.\n",
      "  Batch   320  of  1,490.    Elapsed: 0:02:28.\n",
      "  Batch   360  of  1,490.    Elapsed: 0:02:47.\n",
      "  Batch   400  of  1,490.    Elapsed: 0:03:06.\n",
      "  Batch   440  of  1,490.    Elapsed: 0:03:24.\n",
      "  Batch   480  of  1,490.    Elapsed: 0:03:43.\n",
      "  Batch   520  of  1,490.    Elapsed: 0:04:02.\n",
      "  Batch   560  of  1,490.    Elapsed: 0:04:20.\n",
      "  Batch   600  of  1,490.    Elapsed: 0:04:39.\n",
      "  Batch   640  of  1,490.    Elapsed: 0:04:58.\n",
      "  Batch   680  of  1,490.    Elapsed: 0:05:16.\n",
      "  Batch   720  of  1,490.    Elapsed: 0:05:35.\n",
      "  Batch   760  of  1,490.    Elapsed: 0:05:53.\n",
      "  Batch   800  of  1,490.    Elapsed: 0:06:12.\n",
      "  Batch   840  of  1,490.    Elapsed: 0:06:31.\n",
      "  Batch   880  of  1,490.    Elapsed: 0:06:49.\n",
      "  Batch   920  of  1,490.    Elapsed: 0:07:08.\n",
      "  Batch   960  of  1,490.    Elapsed: 0:07:27.\n",
      "  Batch 1,000  of  1,490.    Elapsed: 0:07:45.\n",
      "  Batch 1,040  of  1,490.    Elapsed: 0:08:04.\n",
      "  Batch 1,080  of  1,490.    Elapsed: 0:08:23.\n",
      "  Batch 1,120  of  1,490.    Elapsed: 0:08:41.\n",
      "  Batch 1,160  of  1,490.    Elapsed: 0:09:00.\n",
      "  Batch 1,200  of  1,490.    Elapsed: 0:09:19.\n",
      "  Batch 1,240  of  1,490.    Elapsed: 0:09:37.\n",
      "  Batch 1,280  of  1,490.    Elapsed: 0:09:56.\n",
      "  Batch 1,320  of  1,490.    Elapsed: 0:10:15.\n",
      "  Batch 1,360  of  1,490.    Elapsed: 0:10:33.\n",
      "  Batch 1,400  of  1,490.    Elapsed: 0:10:52.\n",
      "  Batch 1,440  of  1,490.    Elapsed: 0:11:11.\n",
      "  Batch 1,480  of  1,490.    Elapsed: 0:11:29.\n",
      "\n",
      "  Average training loss: 0.48\n",
      "  Training epcoh took: 0:11:34\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation Loss: 0.43\n",
      "  Validation took: 0:00:23\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,490.    Elapsed: 0:00:19.\n",
      "  Batch    80  of  1,490.    Elapsed: 0:00:37.\n",
      "  Batch   120  of  1,490.    Elapsed: 0:00:56.\n",
      "  Batch   160  of  1,490.    Elapsed: 0:01:15.\n",
      "  Batch   200  of  1,490.    Elapsed: 0:01:33.\n",
      "  Batch   240  of  1,490.    Elapsed: 0:01:52.\n",
      "  Batch   280  of  1,490.    Elapsed: 0:02:11.\n",
      "  Batch   320  of  1,490.    Elapsed: 0:02:29.\n",
      "  Batch   360  of  1,490.    Elapsed: 0:02:48.\n",
      "  Batch   400  of  1,490.    Elapsed: 0:03:07.\n",
      "  Batch   440  of  1,490.    Elapsed: 0:03:25.\n",
      "  Batch   480  of  1,490.    Elapsed: 0:03:44.\n",
      "  Batch   520  of  1,490.    Elapsed: 0:04:03.\n",
      "  Batch   560  of  1,490.    Elapsed: 0:04:22.\n",
      "  Batch   600  of  1,490.    Elapsed: 0:04:40.\n",
      "  Batch   640  of  1,490.    Elapsed: 0:04:59.\n",
      "  Batch   680  of  1,490.    Elapsed: 0:05:18.\n",
      "  Batch   720  of  1,490.    Elapsed: 0:05:36.\n",
      "  Batch   760  of  1,490.    Elapsed: 0:05:55.\n",
      "  Batch   800  of  1,490.    Elapsed: 0:06:14.\n",
      "  Batch   840  of  1,490.    Elapsed: 0:06:32.\n",
      "  Batch   880  of  1,490.    Elapsed: 0:06:51.\n",
      "  Batch   920  of  1,490.    Elapsed: 0:07:10.\n",
      "  Batch   960  of  1,490.    Elapsed: 0:07:28.\n",
      "  Batch 1,000  of  1,490.    Elapsed: 0:07:47.\n",
      "  Batch 1,040  of  1,490.    Elapsed: 0:08:06.\n",
      "  Batch 1,080  of  1,490.    Elapsed: 0:08:25.\n",
      "  Batch 1,120  of  1,490.    Elapsed: 0:08:43.\n",
      "  Batch 1,160  of  1,490.    Elapsed: 0:09:02.\n",
      "  Batch 1,200  of  1,490.    Elapsed: 0:09:21.\n",
      "  Batch 1,240  of  1,490.    Elapsed: 0:09:39.\n",
      "  Batch 1,280  of  1,490.    Elapsed: 0:09:58.\n",
      "  Batch 1,320  of  1,490.    Elapsed: 0:10:17.\n",
      "  Batch 1,360  of  1,490.    Elapsed: 0:10:35.\n",
      "  Batch 1,400  of  1,490.    Elapsed: 0:10:54.\n",
      "  Batch 1,440  of  1,490.    Elapsed: 0:11:13.\n",
      "  Batch 1,480  of  1,490.    Elapsed: 0:11:32.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epcoh took: 0:11:36\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.78\n",
      "  Validation Loss: 0.62\n",
      "  Validation took: 0:00:23\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,490.    Elapsed: 0:00:19.\n",
      "  Batch    80  of  1,490.    Elapsed: 0:00:37.\n",
      "  Batch   120  of  1,490.    Elapsed: 0:00:56.\n",
      "  Batch   160  of  1,490.    Elapsed: 0:01:15.\n",
      "  Batch   200  of  1,490.    Elapsed: 0:01:33.\n",
      "  Batch   240  of  1,490.    Elapsed: 0:01:52.\n",
      "  Batch   280  of  1,490.    Elapsed: 0:02:10.\n",
      "  Batch   320  of  1,490.    Elapsed: 0:02:29.\n",
      "  Batch   360  of  1,490.    Elapsed: 0:02:48.\n",
      "  Batch   400  of  1,490.    Elapsed: 0:03:06.\n",
      "  Batch   440  of  1,490.    Elapsed: 0:03:25.\n",
      "  Batch   480  of  1,490.    Elapsed: 0:03:44.\n",
      "  Batch   520  of  1,490.    Elapsed: 0:04:02.\n",
      "  Batch   560  of  1,490.    Elapsed: 0:04:21.\n",
      "  Batch   600  of  1,490.    Elapsed: 0:04:40.\n",
      "  Batch   640  of  1,490.    Elapsed: 0:04:58.\n",
      "  Batch   680  of  1,490.    Elapsed: 0:05:17.\n",
      "  Batch   720  of  1,490.    Elapsed: 0:05:35.\n",
      "  Batch   760  of  1,490.    Elapsed: 0:05:54.\n",
      "  Batch   800  of  1,490.    Elapsed: 0:06:13.\n",
      "  Batch   840  of  1,490.    Elapsed: 0:06:31.\n",
      "  Batch   880  of  1,490.    Elapsed: 0:06:50.\n",
      "  Batch   920  of  1,490.    Elapsed: 0:07:09.\n",
      "  Batch   960  of  1,490.    Elapsed: 0:07:27.\n",
      "  Batch 1,000  of  1,490.    Elapsed: 0:07:46.\n",
      "  Batch 1,040  of  1,490.    Elapsed: 0:08:05.\n",
      "  Batch 1,080  of  1,490.    Elapsed: 0:08:23.\n",
      "  Batch 1,120  of  1,490.    Elapsed: 0:08:42.\n",
      "  Batch 1,160  of  1,490.    Elapsed: 0:09:01.\n",
      "  Batch 1,200  of  1,490.    Elapsed: 0:09:19.\n",
      "  Batch 1,240  of  1,490.    Elapsed: 0:09:38.\n",
      "  Batch 1,280  of  1,490.    Elapsed: 0:09:56.\n",
      "  Batch 1,320  of  1,490.    Elapsed: 0:10:15.\n",
      "  Batch 1,360  of  1,490.    Elapsed: 0:10:34.\n",
      "  Batch 1,400  of  1,490.    Elapsed: 0:10:52.\n",
      "  Batch 1,440  of  1,490.    Elapsed: 0:11:11.\n",
      "  Batch 1,480  of  1,490.    Elapsed: 0:11:30.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epcoh took: 0:11:34\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.79\n",
      "  Validation Loss: 0.76\n",
      "  Validation took: 0:00:23\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,490.    Elapsed: 0:00:19.\n",
      "  Batch    80  of  1,490.    Elapsed: 0:00:37.\n",
      "  Batch   120  of  1,490.    Elapsed: 0:00:56.\n",
      "  Batch   160  of  1,490.    Elapsed: 0:01:14.\n",
      "  Batch   200  of  1,490.    Elapsed: 0:01:33.\n",
      "  Batch   240  of  1,490.    Elapsed: 0:01:52.\n",
      "  Batch   280  of  1,490.    Elapsed: 0:02:10.\n",
      "  Batch   320  of  1,490.    Elapsed: 0:02:29.\n",
      "  Batch   360  of  1,490.    Elapsed: 0:02:47.\n",
      "  Batch   400  of  1,490.    Elapsed: 0:03:06.\n",
      "  Batch   440  of  1,490.    Elapsed: 0:03:25.\n",
      "  Batch   480  of  1,490.    Elapsed: 0:03:43.\n",
      "  Batch   520  of  1,490.    Elapsed: 0:04:02.\n",
      "  Batch   560  of  1,490.    Elapsed: 0:04:20.\n",
      "  Batch   600  of  1,490.    Elapsed: 0:04:39.\n",
      "  Batch   640  of  1,490.    Elapsed: 0:04:58.\n",
      "  Batch   680  of  1,490.    Elapsed: 0:05:16.\n",
      "  Batch   720  of  1,490.    Elapsed: 0:05:35.\n",
      "  Batch   760  of  1,490.    Elapsed: 0:05:54.\n",
      "  Batch   800  of  1,490.    Elapsed: 0:06:12.\n",
      "  Batch   840  of  1,490.    Elapsed: 0:06:31.\n",
      "  Batch   880  of  1,490.    Elapsed: 0:06:49.\n",
      "  Batch   920  of  1,490.    Elapsed: 0:07:08.\n",
      "  Batch   960  of  1,490.    Elapsed: 0:07:26.\n",
      "  Batch 1,000  of  1,490.    Elapsed: 0:07:45.\n",
      "  Batch 1,040  of  1,490.    Elapsed: 0:08:04.\n",
      "  Batch 1,080  of  1,490.    Elapsed: 0:08:22.\n",
      "  Batch 1,120  of  1,490.    Elapsed: 0:08:41.\n",
      "  Batch 1,160  of  1,490.    Elapsed: 0:08:59.\n",
      "  Batch 1,200  of  1,490.    Elapsed: 0:09:18.\n",
      "  Batch 1,240  of  1,490.    Elapsed: 0:09:37.\n",
      "  Batch 1,280  of  1,490.    Elapsed: 0:09:55.\n",
      "  Batch 1,320  of  1,490.    Elapsed: 0:10:14.\n",
      "  Batch 1,360  of  1,490.    Elapsed: 0:10:33.\n",
      "  Batch 1,400  of  1,490.    Elapsed: 0:10:51.\n",
      "  Batch 1,440  of  1,490.    Elapsed: 0:11:10.\n",
      "  Batch 1,480  of  1,490.    Elapsed: 0:11:28.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epcoh took: 0:11:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.79\n",
      "  Validation Loss: 0.93\n",
      "  Validation took: 0:00:23\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:47:49 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ye_2QExHGBg0",
    "outputId": "1d259852-8c55-4b32-ac02-e13552fe052a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,490.    Elapsed: 0:00:19.\n",
      "  Batch    80  of  1,490.    Elapsed: 0:00:38.\n",
      "  Batch   120  of  1,490.    Elapsed: 0:00:56.\n",
      "  Batch   160  of  1,490.    Elapsed: 0:01:15.\n",
      "  Batch   200  of  1,490.    Elapsed: 0:01:34.\n",
      "  Batch   240  of  1,490.    Elapsed: 0:01:53.\n",
      "  Batch   280  of  1,490.    Elapsed: 0:02:11.\n",
      "  Batch   320  of  1,490.    Elapsed: 0:02:30.\n",
      "  Batch   360  of  1,490.    Elapsed: 0:02:49.\n",
      "  Batch   400  of  1,490.    Elapsed: 0:03:08.\n",
      "  Batch   440  of  1,490.    Elapsed: 0:03:26.\n",
      "  Batch   480  of  1,490.    Elapsed: 0:03:45.\n",
      "  Batch   520  of  1,490.    Elapsed: 0:04:04.\n",
      "  Batch   560  of  1,490.    Elapsed: 0:04:23.\n",
      "  Batch   600  of  1,490.    Elapsed: 0:04:42.\n",
      "  Batch   640  of  1,490.    Elapsed: 0:05:00.\n",
      "  Batch   680  of  1,490.    Elapsed: 0:05:19.\n",
      "  Batch   720  of  1,490.    Elapsed: 0:05:38.\n",
      "  Batch   760  of  1,490.    Elapsed: 0:05:57.\n",
      "  Batch   800  of  1,490.    Elapsed: 0:06:15.\n",
      "  Batch   840  of  1,490.    Elapsed: 0:06:34.\n",
      "  Batch   880  of  1,490.    Elapsed: 0:06:53.\n",
      "  Batch   920  of  1,490.    Elapsed: 0:07:12.\n",
      "  Batch   960  of  1,490.    Elapsed: 0:07:31.\n",
      "  Batch 1,000  of  1,490.    Elapsed: 0:07:49.\n",
      "  Batch 1,040  of  1,490.    Elapsed: 0:08:08.\n",
      "  Batch 1,080  of  1,490.    Elapsed: 0:08:27.\n",
      "  Batch 1,120  of  1,490.    Elapsed: 0:08:46.\n",
      "  Batch 1,160  of  1,490.    Elapsed: 0:09:04.\n",
      "  Batch 1,200  of  1,490.    Elapsed: 0:09:23.\n",
      "  Batch 1,240  of  1,490.    Elapsed: 0:09:42.\n",
      "  Batch 1,280  of  1,490.    Elapsed: 0:10:01.\n",
      "  Batch 1,320  of  1,490.    Elapsed: 0:10:20.\n",
      "  Batch 1,360  of  1,490.    Elapsed: 0:10:38.\n",
      "  Batch 1,400  of  1,490.    Elapsed: 0:10:57.\n",
      "  Batch 1,440  of  1,490.    Elapsed: 0:11:16.\n",
      "  Batch 1,480  of  1,490.    Elapsed: 0:11:35.\n",
      "\n",
      "  Average training loss: 4.99\n",
      "  Training epcoh took: 0:11:39\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.07\n",
      "  Validation Loss: 5.23\n",
      "  Validation took: 0:00:23\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,490.    Elapsed: 0:00:19.\n",
      "  Batch    80  of  1,490.    Elapsed: 0:00:37.\n",
      "  Batch   120  of  1,490.    Elapsed: 0:00:56.\n",
      "  Batch   160  of  1,490.    Elapsed: 0:01:15.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-5b624170541a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# single value; the `.item()` function just returns the Python value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# from the tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Perform a backward pass to calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats2 = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader2):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader2), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader2)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader2:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader2)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader2)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats2.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQTvJ1vRP7u4"
   },
   "source": [
    "Let's view the summary of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "6O_NbXFGMukX",
    "outputId": "3a08a631-8e13-488d-c4f7-aeb23edbc9f3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0:11:34</td>\n",
       "      <td>0:00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0:11:36</td>\n",
       "      <td>0:00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0:11:34</td>\n",
       "      <td>0:00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0:11:33</td>\n",
       "      <td>0:00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.48         0.43           0.80       0:11:34         0:00:23\n",
       "2               0.37         0.62           0.78       0:11:36         0:00:23\n",
       "3               0.29         0.76           0.79       0:11:34         0:00:23\n",
       "4               0.20         0.93           0.79       0:11:33         0:00:23"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "68xreA9JAmG5",
    "outputId": "a323ed72-0d75-4896-bf6a-7f13c4b8990a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkyubuJSOzg3"
   },
   "source": [
    "# 5. Performance On Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DosV94BYIYxg"
   },
   "source": [
    "Now we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using [Matthew's correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tg42jJqqM68F"
   },
   "source": [
    "### 5.1. Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWe0_JW21MyV"
   },
   "source": [
    "\n",
    "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mAN0LZBOOPVh",
    "outputId": "cc2eef73-7f08-4344-e70f-ca404d1d3d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 861\n",
      "\n",
      "id\ttweet\n",
      "['OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF'\n",
      " 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
      " 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
      " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT'\n",
      " 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
      " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'OFF' 'NOT'\n",
      " 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT'\n",
      " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT'\n",
      " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'OFF'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF'\n",
      " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT'\n",
      " 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF'\n",
      " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'OFF' 'OFF' 'NOT'\n",
      " 'OFF' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
      " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'OFF'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'OFF'\n",
      " 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT'\n",
      " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT'\n",
      " 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF'\n",
      " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT'\n",
      " 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF'\n",
      " 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF'\n",
      " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'OFF'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
      " 'NOT' 'OFF' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT'\n",
      " 'NOT' 'OFF' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
      " 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT'\n",
      " 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'OFF'\n",
      " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT'\n",
      " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
      " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'OFF'\n",
      " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
      " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT']\n",
      "\n",
      "860\n",
      "0\n",
      "torch.Size([860, 64])\n",
      "tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df1 = pd.read_csv(\"./testset-levela.tsv\", delimiter=',', header=None, names=['tweet'])\n",
    "df2 = pd.read_csv(\"./labels-levela.csv\", delimiter=',', header=None, names=['label'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df1.shape[0]))\n",
    "print(sentences[0])\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df1.tweet.values\n",
    "labels = df2.label.values\n",
    "label=[]\n",
    "print(labels)\n",
    "for i in range(len(labels)):\n",
    "      if labels[i]=='OFF':\n",
    "        label.append(0)\n",
    "      else:\n",
    "        label.append(1)\n",
    "print()\n",
    "print(len(label))\n",
    "print(label[0])\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences[1:861]:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "print(input_ids.shape)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(label)\n",
    "print(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16lctEOyNFik"
   },
   "source": [
    "## 5.2. Evaluate on Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhR99IISNMg9"
   },
   "source": [
    "\n",
    "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvqBYratjnLz"
   },
   "outputs": [],
   "source": [
    "def truepositive(preds, labels):\n",
    "    tp=0\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    #print(pred_flat[0])\n",
    "    #print(labels_flat)\n",
    "   # pred=sum(pred_flat)\n",
    "   # print(pred)\n",
    "    for i in range(len(labels_flat)):\n",
    "     # print(pred_flat[i])\n",
    "      if pred_flat[i]==1 & labels_flat[i]==1:\n",
    "        tp=tp+1\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4sOMTyhluv6Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Hba10sXR7Xi6",
    "outputId": "9d0376de-6fd3-424a-fff3-9eb63c733357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 13,241 test sentences...\n",
      "0.8538812785388128\n",
      "0.9048387096774193\n",
      "0.8786217697729053\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "total_eval_accuracy=0\n",
    "Tp=0\n",
    "Pred=0\n",
    "Ap=0\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "Test_stats=[]\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  #predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "  pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "  pred=sum(pred_flat)\n",
    "  labels_flat = label_ids.flatten()\n",
    "  Ap+=sum(labels_flat)\n",
    "  #predictions.append(pred_flat)\n",
    "  total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "  tp=truepositive(logits, label_ids)\n",
    "  Tp=Tp+tp\n",
    "  Pred=pred+Pred\n",
    "  #print(Pred)\n",
    " \n",
    "\n",
    "\n",
    "#predictions = torch.tensor(predictions)\n",
    "#print(sum(predictions))\n",
    "avg_test_accuracy = total_eval_accuracy / len(prediction_dataloader)\n",
    "Percision=Tp/Pred\n",
    "Recall=Tp/Ap\n",
    "F1=2*Percision*Recall/(Percision+Recall)\n",
    "print(Percision)\n",
    "print(Recall)\n",
    "print(F1)\n",
    "#print(\"  Accuracy: {0:.2f}\".format(avg_test_accuracy))\n",
    "#print(\"  Recall: {0:.2f}\".format())\n",
    "Test_stats.append(\n",
    "        {\n",
    "\n",
    "            'Percision': Percision,\n",
    "            'Recall': Recall,\n",
    "            'F1-score': F1,\n",
    "            'avg_test_accuracy':avg_test_accuracy\n",
    "         \n",
    "\n",
    "        }\n",
    ")\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "eKc13P1UE3WY",
    "outputId": "1c0fba86-8463-4e22-9566-61910e11071b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>avg_test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Percision  Recall  F1-score  avg_test_accuracy\n",
       "0       0.85     0.9      0.88               0.82"
      ]
     },
     "execution_count": 177,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=Test_stats)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5jscIM8R4Gv"
   },
   "source": [
    "Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n",
    "\n",
    "We use MCC here because the classes are imbalanced:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "cRaZQ4XC7kLs",
    "outputId": "22d3d713-ac73-4b7e-bcb6-68e3493cc324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  \n",
    "  # Calculate and store the coef for this batch.  \n",
    "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "  matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUM0UA1qJaVB"
   },
   "source": [
    "The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches. \n",
    "\n",
    "Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "pyfY1tqxU0t9",
    "outputId": "9a8166c7-eb68-45ab-be67-db6bfeef1529"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a barplot showing the MCC score for each batch of test samples.\n",
    "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
    "\n",
    "plt.title('MCC Score per Batch')\n",
    "plt.ylabel('MCC Score (-1 to +1)')\n",
    "plt.xlabel('Batch #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YrjAPX2V-l4"
   },
   "source": [
    "Now we'll combine the results for all of the batches and calculate our final MCC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "oCYZa1lQ8Jn8",
    "outputId": "775f0853-7350-4ea6-df17-9e45b6c9ef55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MCC: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('Total MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jXx0jPc4HUfZ"
   },
   "source": [
    "Cool! In about half an hour and without doing any hyperparameter tuning (adjusting the learning rate, epochs, batch size, ADAM properties, etc.) we are able to get a good score. \n",
    "\n",
    "> *Note: To maximize the score, we should remove the \"validation set\" (which we used to help determine how many epochs to train for) and train on the entire training set.*\n",
    "\n",
    "The library documents the expected accuracy for this benchmark [here](https://huggingface.co/transformers/examples.html#glue) as `49.23`.\n",
    "\n",
    "You can also look at the official leaderboard [here](https://gluebenchmark.com/leaderboard/submission/zlssuBTm5XRs0aSKbFYGVIVdvbj1/-LhijX9VVmvJcvzKymxy). \n",
    "\n",
    "Note that (due to the small dataset size?) the accuracy can vary significantly between runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GfjYoa6WmkN6"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xlQG7qgkmf4n"
   },
   "source": [
    "This post demonstrates that with a pre-trained BERT model you can quickly and effectively create a high quality model with minimal effort and training time using the pytorch interface, regardless of the specific NLP task you are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUmsUOIv8EUO"
   },
   "source": [
    "# Appendix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2079Qyn8Mt8"
   },
   "source": [
    "## A1. Saving & Loading Fine-Tuned Model\n",
    "\n",
    "This first cell (taken from `run_glue.py` [here](https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495)) writes the model and tokenizer out to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "6ulTWaOr8QNY",
    "outputId": "bb6c10b4-e5ab-42c1-c07c-232b42ddc451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/vocab.txt',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-tjHkR7lc1I"
   },
   "source": [
    "Let's check out the file sizes, out of curiosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "mqMzI3VTCZo5",
    "outputId": "678048d5-4631-4e89-9f66-2a4bc27ba611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 427960K\n",
      "-rw-r--r-- 1 root root      2K Apr  8 19:26 config.json\n",
      "-rw-r--r-- 1 root root 427719K Apr  8 19:26 pytorch_model.bin\n",
      "-rw-r--r-- 1 root root      1K Apr  8 19:26 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root      1K Apr  8 19:26 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root    227K Apr  8 19:26 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=K ./model_save/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fr_bt2rFlgDn"
   },
   "source": [
    "The largest file is the model weights, at around 418 megabytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-WUFUIQ8Cu8D",
    "outputId": "70780762-7790-474f-e5c2-304a066945ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 418M Mar 18 15:53 ./model_save/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=M ./model_save/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzGKvOFAll_e"
   },
   "source": [
    "To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Trr-A-POC18_"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to this Notebook instance.\n",
    "from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxlZsafTC-V5"
   },
   "outputs": [],
   "source": [
    "# Copy the model files to a directory in your Google Drive.\n",
    "!cp -r ./model_save/ \"./drive/Shared drives/ChrisMcCormick.AI/Blog Posts/BERT Fine-Tuning/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0vstijw85SZ"
   },
   "source": [
    "The following functions will load the model back from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nskPzUM084zL"
   },
   "outputs": [],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = model_class.from_pretrained(output_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIWouvDrGVAi"
   },
   "source": [
    "## A.2. Weight Decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f123ZAlF1OyW"
   },
   "source": [
    "The huggingface example includes the following code block for enabling weight decay, but the default decay rate is \"0.0\", so I moved this to the appendix.\n",
    "\n",
    "This block essentially tells the optimizer to not apply weight decay to the bias terms (e.g., $ b $ in the equation $ y = Wx + b $ ). Weight decay is a form of regularization--after calculating the gradients, we multiply them by, e.g., 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxSMw0FrptiL"
   },
   "outputs": [],
   "source": [
    "# This code is taken from:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n",
    "\n",
    "# Don't apply weight decay to any parameters whose names include these tokens.\n",
    "# (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# Separate the `weight` parameters from the `bias` parameters. \n",
    "# - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. \n",
    "# - For the `bias` parameters, the 'weight_decay_rate' is 0.0. \n",
    "optimizer_grouped_parameters = [\n",
    "    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.1},\n",
    "    \n",
    "    # Filter for parameters which *do* include those.\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "# Note - `optimizer_grouped_parameters` only includes the parameter values, not \n",
    "# the names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKzLS9ohzGVu"
   },
   "source": [
    "# Revision History"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of BERT Fine-Tuning Sentence Classification v3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "059082807ff446f196a83a8844aa6bc0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05d6b1d7982644e5b4db88f048035ba7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13f7e3eb985047edb280c039c9e9d5d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93efcaeb7b5040f9a5e926fff547d16e",
      "placeholder": "​",
      "style": "IPY_MODEL_05d6b1d7982644e5b4db88f048035ba7",
      "value": " 433/433 [00:01&lt;00:00, 265B/s]"
     }
    },
    "2115a9df3e2744908f9e4baae71e123d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e383abb30458425a8fc7105b4fd1fdd7",
      "placeholder": "​",
      "style": "IPY_MODEL_c98c6d5271c24f7bb171527accc7a604",
      "value": " 440M/440M [00:46&lt;00:00, 9.55MB/s]"
     }
    },
    "49d3727caad24f10aa0cea67fe3e513f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5c3b6f5b995043b7b0f9ec3bb7d271af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f011e997a0244e9b4c411ed126c5bf2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "884cc5bf6ece437dbfb8221b3f90a549": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c3b6f5b995043b7b0f9ec3bb7d271af",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_49d3727caad24f10aa0cea67fe3e513f",
      "value": 440473133
     }
    },
    "93efcaeb7b5040f9a5e926fff547d16e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a0a6d8a4ea14081957c1c6931e36516": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb0be6ae80a54283a34860502f77ffc8",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb0602500cef422ebc91c6b61a53c242",
      "value": 433
     }
    },
    "b7f400b9b8774c6ca6e08f6fa67cdbaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_884cc5bf6ece437dbfb8221b3f90a549",
       "IPY_MODEL_2115a9df3e2744908f9e4baae71e123d"
      ],
      "layout": "IPY_MODEL_059082807ff446f196a83a8844aa6bc0"
     }
    },
    "c98c6d5271c24f7bb171527accc7a604": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb0602500cef422ebc91c6b61a53c242": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cb0be6ae80a54283a34860502f77ffc8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d99d4da3fd52476b97df5fa5fb56e632": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a0a6d8a4ea14081957c1c6931e36516",
       "IPY_MODEL_13f7e3eb985047edb280c039c9e9d5d0"
      ],
      "layout": "IPY_MODEL_6f011e997a0244e9b4c411ed126c5bf2"
     }
    },
    "e383abb30458425a8fc7105b4fd1fdd7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
