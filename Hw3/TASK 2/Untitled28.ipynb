{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model_skeleton.py\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ParserModel(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, config, word_embeddings=None, pos_embeddings=None,\n",
    "                 dep_embeddings=None):\n",
    "        self.config = config\n",
    "        # These are the hyper-parameters for choosing how many embeddings to\n",
    "        # encode in the input layer.  See the last paragraph of 3.1\n",
    "        n_w = config.word_features_types # 18\n",
    "        n_p = config.pos_features_types # 18\n",
    "        n_d = config.dep_features_types # 12\n",
    "        # Copy the Embedding data that we'll be using in the model.  Note that the\n",
    "        # model gets these in the constructor so that the embeddings can come\n",
    "        # from anywhere (the model is agnostic to the source of the embeddings).\n",
    "        self.word_embeddings = nn.Embedding(config.word_vocab_size,\n",
    "                                       config.embedding_dim)\n",
    "        self.pos_embeddings = nn.Embedding(config.pos_vocab_size,\n",
    "                                      config.embedding_dim)\n",
    "        self.dep_embeddings = nn.Embedding(config.word_vocab_size,\n",
    "                                       config.embedding_dim)\n",
    "        # Create the first layer of the network that transform the input data\n",
    "        # (consisting of embeddings of words, their corresponding POS tags, and\n",
    "        # the arc labels) to the hidden layer raw outputs.\n",
    "        self.embed_to_hidden = nn.Linear(config.embedding_dim*(n_w+n_p+n_d, config.hidden_size))\n",
    "        \n",
    "\n",
    "        # TODO\n",
    "\n",
    "        \n",
    "        # After the activation of the hidden layer, you'll be randomly zero-ing\n",
    "        # out a percentage of the activations, which is a process known as\n",
    "        # \"Dropout\".  Dropout helps the model avoid looking at the activation of\n",
    "        # one particular neuron and be more robust.  (In essence, dropout is\n",
    "        # turning the one network into an *ensemble* of networks).  Create a\n",
    "        # Dropout layer here that we'll use later in the forward() call.\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "\n",
    "        # TODO\n",
    "        # Create the output layer that maps the activation of the hidden layer to\n",
    "        # the output classes (i.e., the valid transitions)\n",
    "        self.hidden_to_logits = nn.Linear(config.hidden_size, config.n_classes)\n",
    "\n",
    "        # TODO\n",
    "\n",
    "        # Initialize the weights of both layers\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # initialize each layer's weights to be uniformly distributed within this\n",
    "        # range of +/-initrange.  This initialization ensures the weights have something to\n",
    "        # start with for computing gradient descent and generally leads to\n",
    "        # faster convergence.\n",
    "        initrange = 0.1\n",
    "        init.xavier_uniform_(self.embed_to_hidden.weight)\n",
    "        init.xavier_uniform_(self.hidden_to_logits.weight)\n",
    "\n",
    "    def lookup_embeddings(self, word_indices, pos_indices, dep_indices, keep_pos = 1):\n",
    "        \n",
    "        # Based on the IDs, look up the embeddings for each thing we need.  Note\n",
    "        # that the indices are a list of which embeddings need to be returned.\n",
    "        w_embeddings=self.word_embeddings(word_indices)\n",
    "        p_embeddings=self.pos_embeddings(pos_indices)\n",
    "        d_embeddings=self.dep_embeddings(dep_indices)\n",
    "        x = x.view(-1, self.n_features * self.embed_size) \n",
    "\n",
    "        # TODO\n",
    "        \n",
    "        return w_embeddings, p_embeddings, d_embeddings\n",
    "\n",
    "    def forward(self, word_indices, pos_indices, dep_indices):\n",
    "        \"\"\"\n",
    "        Computes the next transition step (shift, reduce-left, reduce-right)\n",
    "        based on the current state of the input.\n",
    "        \n",
    "\n",
    "        The indices here represent the words/pos/dependencies in the current\n",
    "        context, which we'll need to turn into vectors.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Look up the embeddings for this prediction.  Note that word_indices is\n",
    "        # the list of certain words currently on the stack and buffer, rather\n",
    "        # than a single word\n",
    "        w_embeddings, p_embeddings, d_embeddings = self.lookup_embeddings(word_indices,pos_indices,dep_indices)\n",
    "        # TODO\n",
    "\n",
    "        # Since we're converting lists of indices, we're getting a matrix back\n",
    "        # out (each index becomes a vector).  We need to turn these into\n",
    "        # single-dimensional vector (Flatten each of the embeddings into a\n",
    "        # single dimension).  Note that the first dimension is the batch.  For\n",
    "        # example, if we have a batch size of 2, 3 words per context, and 5\n",
    "        # dimensions per embedding, word_embeddings should be tensor with size\n",
    "        # (2,3,5).  We need it to be a tensor with size (2,15), which makes the\n",
    "        # input just like that flat input vector you see in the network diagram.\n",
    "        #\n",
    "        # HINT: you don't need to copy data here, only reshape the tensor.\n",
    "        # Functions like \"view\" (similar to numpy's \"reshape\" function will be\n",
    "        # useful here.        \n",
    "\n",
    "        # TODO\n",
    "        w_embeddings = w_embeddings.view(-1,  self.config.word_features_types * self.config.embedding_dim) \n",
    "        p_embeddings = p_embeddings.view(-1,  self.config.word_features_types * self.config.embedding_dim) \n",
    "        d_embeddings = d_embeddings.view(-1,  self.config.word_features_types * self.config.embedding_dim) \n",
    "                                         \n",
    "        \n",
    "        # Compute the raw hidden layer activations from the concatentated input\n",
    "        # embeddings.\n",
    "        #\n",
    "        # NOTE: if you're attempting the optional parts where you want to\n",
    "        # compute separate weight matrices for each type of input, you'll need\n",
    "        # do this step for each one!\n",
    "        embeddings=torch.cat(w_embeddings,p_embeddings,d_embeddings,0)                                 \n",
    "        t = self.embed_to_hidden(embeddings)\n",
    "\n",
    "        # TODO\n",
    "        \n",
    "        \n",
    "        # Compute the cubic activation function here.\n",
    "        #\n",
    "        # NOTE: Pytorch doesn't have a cubic activation function in the library\n",
    "\n",
    "        # TODO\n",
    "        t = torch.pow(t,2)\n",
    "        \n",
    "\n",
    "        # Now do dropout for final activations of the first hidden layer\n",
    "        t = self.dropout(t)\n",
    "\n",
    "        # TODO\n",
    "\n",
    "        # Multiply the activation of the first hidden layer by the weights of\n",
    "        # the second hidden layer and pass that through a ReLU non-linearity for\n",
    "        # the final output activations.\n",
    "        #\n",
    "        # NOTE 1: this output does not need to be pushed through a softmax if\n",
    "        # you're going to evaluate the output using the CrossEntropy loss\n",
    "        # function, which will compute the softmax intrinsically as a part of\n",
    "        # its optimization when computing the loss.\n",
    "\n",
    "        # TODO\n",
    "        t = self.hidden_to_logits(t)\n",
    "        output= F.relu(t)\n",
    "\n",
    "        return output    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
