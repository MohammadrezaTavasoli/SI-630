{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--max_train_iters MAX_TRAIN_ITERS]\n",
      "                             [--parser_name PARSER_NAME]\n",
      "                             [--num_epochs NUM_EPOCHS]\n",
      "                             [--print_every_iters PRINT_EVERY_ITERS] [--train]\n",
      "                             [--test] [--load_model_file LOAD_MODEL_FILE]\n",
      "                             [--parse_sentence PARSE_SENTENCE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/a/Library/Jupyter/runtime/kernel-ee2d9ded-1cb1-49e1-b343-f39571465691.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# %load main_skeleton.py\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from feature_extraction import (NUM_DEPS, SHIFT, DataConfig, Flags,\n",
    "                                load_datasets, pos_prefix, punc_pos)\n",
    "from general_utils import get_minibatches\n",
    "#from model import ParserModel\n",
    "from test_functions import compute_dependencies, get_UAS, parse_sentence\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def load_embeddings(config, emb_type='new', emb_file_name=None):\n",
    "    if emb_type == 'new':\n",
    "        print('Creating new trainable embeddings')\n",
    "        word_embeddings = nn.Embedding(config.word_vocab_size,\n",
    "                                       config.embedding_dim)\n",
    "        pos_embeddings = nn.Embedding(config.pos_vocab_size,\n",
    "                                      config.embedding_dim)\n",
    "        dep_embeddings = nn.Embedding(config.dep_vocab_size,\n",
    "                                      config.embedding_dim)\n",
    "    elif emb_type == 'twitter':\n",
    "        # TODO\n",
    "        pass\n",
    "    elif emb_type == 'wiki' or emb_type == 'wikipedia':\n",
    "        # TODO\n",
    "        pass\n",
    "    else:\n",
    "        raise Error('unknown embedding type!: \"%s\"' % emb_type)\n",
    "\n",
    "    return word_embeddings, pos_embeddings, dep_embeddings\n",
    "\n",
    "\n",
    "def train(save_dir='saved_weights',\n",
    "          parser_name='parser',\n",
    "          num_epochs=5,\n",
    "          max_iters=-1,\n",
    "          print_every_iters=10):\n",
    "    \"\"\"\n",
    "    Trains the model.\n",
    "\n",
    "    parser_name is the string prefix used for the filename where the parser is\n",
    "    saved after every epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # load dataset\n",
    "    load_existing_dump = False\n",
    "    print('Loading dataset for training')\n",
    "    dataset = load_datasets(load_existing_dump)\n",
    "    # HINT: Look in the ModelConfig class for the model's hyperparameters\n",
    "    config = dataset.model_config\n",
    "\n",
    "    print('Loading embeddings')\n",
    "    word_embeddings, pos_embeddings, dep_embeddings = load_embeddings(config)\n",
    "    # TODO: For Task 3, add Twitter and Wikipedia embeddings (do this last)\n",
    "\n",
    "    if False:\n",
    "        # Switch to True if you want to print examples of feature types\n",
    "        print('words: ', len(dataset.word2idx))\n",
    "        print('examples: ', [(k, v)\n",
    "                             for i, (k,\n",
    "                                     v) in enumerate(dataset.word2idx.items())\n",
    "                             if i < 30])\n",
    "        print('\\n')\n",
    "        print('POS-tags: ', len(dataset.pos2idx))\n",
    "        print(dataset.pos2idx)\n",
    "        print('\\n')\n",
    "        print('dependencies: ', len(dataset.dep2idx))\n",
    "        print(dataset.dep2idx)\n",
    "        print('\\n')\n",
    "        print(\"some hyperparameters\")\n",
    "        print(vars(config))\n",
    "\n",
    "    # load parser object\n",
    "    parser = ParserModel(config, word_embeddings, pos_embeddings,\n",
    "                         dep_embeddings)\n",
    "    device = torch.device(\n",
    "        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    parser.to(device)\n",
    "\n",
    "    # set save_dir for model\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # create object for loss function\n",
    "\n",
    "    # TODO\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create object for an optimizer that updated the weights of our parser\n",
    "    # model.  Be sure to set the learning rate based on the parameters!\n",
    "    optimizer = optim.Adam(parser.model.parameters())\n",
    "\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "        ###### Training #####\n",
    "\n",
    "        # load training set in minibatches\n",
    "        for i, (train_x, train_y) in enumerate(get_minibatches([dataset.train_inputs,\n",
    "                                                                dataset.train_targets], \\\n",
    "                                                               config.batch_size,\n",
    "                                                               is_multi_feature_input=True)):\n",
    "\n",
    "            word_inputs_batch, pos_inputs_batch, dep_inputs_batch = train_x\n",
    "\n",
    "            # Convert the numpy data to pytorch's tensor represetation.  They're\n",
    "            # numpy objects initially.  NOTE: In general, when using Pytorch,\n",
    "            # you want to send them to the device that will do th e computation\n",
    "            # (either a GPU or CPU).  You do this by saying \"obj.to(device)\"\n",
    "            # where we've already created the device for you (see above where we\n",
    "            # did this for the parser).  This ensures your data is running on\n",
    "            # the processor you expect it to!\n",
    "            word_inputs_batch = torch.from_numpy(train_x).long()\n",
    "            pos_inputs_batch = torch.from_numpy(train_x).long()  # TODO\n",
    "            dep_inputs_batch = torch.from_numpy(train_x).long()\n",
    "    \n",
    "\n",
    "            # Convert the labels from 1-hot vectors to a list of which index was\n",
    "            # 1, which is what Pytorch expects.  HINT: look for the \"argmax\"\n",
    "            # function in numpy.\n",
    "            labels = torch.from_numpy(train_y.nonzero()[1]).long().to(device)  # TODO\n",
    "\n",
    "            # Convert the label to pytorch's tensor\n",
    "            # TODO\n",
    "\n",
    "            # This is just a quick hack so you can cut training short to see how\n",
    "            # things are working.  In the final model, make sure to use all the data!\n",
    "            if max_iters >= 0 and i > max_iters:\n",
    "                break\n",
    "\n",
    "            # Some debugging information for you\n",
    "            if i == 0 and epoch == 1:\n",
    "                print(\"size of word inputs: \", word_inputs_batch.size())\n",
    "                print(\"size of pos inputs: \", pos_inputs_batch.size())\n",
    "                print(\"size of dep inputs: \", dep_inputs_batch.size())\n",
    "                print(\"size of labels: \", labels.size())\n",
    "\n",
    "            #\n",
    "            #### Backprop & Update weights ####\n",
    "            #\n",
    "              # remove any baggage in the optimizer\n",
    "             # store loss for this batch here\n",
    "\n",
    "            # Before the backward pass, use the optimizer object to zero all of\n",
    "            # the gradients for the variables\n",
    "\n",
    "            # TODO\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            # For the current batch of inputs, run a full forward pass through the\n",
    "            # data and get the outputs for each item's prediction.\n",
    "            # These are the raw outputs, which represent the activations for\n",
    "            # prediction over valid transitions.\n",
    "\n",
    "            outputs = parser.model(train_x)  # TODO\n",
    "\n",
    "            # Compute the loss for the outputs with the labels.  Note that for\n",
    "            # your particular loss (cross-entropy) it will compute the softmax\n",
    "            # for you, so you can safely pass in the raw activations.\n",
    "\n",
    "            loss =loss_func(outputs, train_y)  # TODO\n",
    "\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "\n",
    "            loss.backward()\n",
    "           \n",
    "            # TODO\n",
    "\n",
    "            # Perform 1 update using the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # TODO\n",
    "\n",
    "            # Every 10 batches, print out some reporting so we can see convergence\n",
    "            if i % print_every_iters == 0:\n",
    "                print ('Epoch: %d [%d], loss: %1.3f, acc: %1.3f' \\\n",
    "                       % (epoch, i, loss.item(),\n",
    "                          int((outputs.argmax(1)==labels).sum())/len(labels)))\n",
    "\n",
    "        print(\"End of epoch\")\n",
    "\n",
    "        # save model\n",
    "        save_file = os.path.join(save_dir, '%s-epoch-%d.mdl' % (parser_name,\n",
    "                                                                epoch))\n",
    "        print('Saving current state of model to %s' % save_file)\n",
    "        torch.save(parser, save_file)\n",
    "\n",
    "        ###### Validation #####\n",
    "        print('Evaluating on valudation data after epoch %d' % epoch)\n",
    "\n",
    "        # Once we're in test/validation time, we need to indicate that we are in\n",
    "        # \"evaluation\" mode.  This will turn off things like Dropout so that\n",
    "        # we're not randomly zero-ing out weights when it might hurt performance\n",
    "        parser.eval()\n",
    "\n",
    "        # Compute the current model's UAS score on the validation (development)\n",
    "        # dataset.  Note that we can use this held-out data to tune the\n",
    "        # hyper-parameters of the model but we should never look at the test\n",
    "        # data until we want to report the very final result.\n",
    "        compute_dependencies(parser, device, dataset.valid_data, dataset)\n",
    "        valid_UAS = get_UAS(dataset.valid_data)\n",
    "        print(\"- validation UAS: {:.2f}\".format(valid_UAS * 100.0))\n",
    "\n",
    "        # Once we're done with test/validation, we need to indicate that we are back in\n",
    "        # \"train\" mode.  This will turn back on things like Dropout\n",
    "        parser.train()\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def test(parser):\n",
    "\n",
    "    # load dataset\n",
    "    print('Loading data for testing')\n",
    "    dataset = load_datasets()\n",
    "    config = dataset.model_config\n",
    "    device = torch.device(\n",
    "        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Make sure the parser is in evaluation mode so it's not using things like dropout\n",
    "    parser.eval()\n",
    "\n",
    "    # Compute UAS (unlabeled attachment score), which is the standard evaluate metric for parsers.\n",
    "    #\n",
    "    # For details see\n",
    "    # http://www.morganclaypool.com/doi/abs/10.2200/S00169ED1V01Y200901HLT002\n",
    "    # Chapter 6.1\n",
    "    compute_dependencies(parser, device, dataset.test_data, dataset)\n",
    "    valid_UAS = get_UAS(dataset.test_data)\n",
    "    print(\"- test UAS: {:.2f}\".format(valid_UAS * 100.0))\n",
    "\n",
    "    parser.eval()\n",
    "    test_string = \"I shot an elephant with a banana\"\n",
    "    parse_sentence(test_string, parser, device, dataset)\n",
    "\n",
    "def parse_example(parser, sentence):\n",
    "\n",
    "    # load dataset\n",
    "    print('Loading embeddings and ids for parsing')\n",
    "    dataset = load_datasets()\n",
    "    config = dataset.model_config\n",
    "    device = torch.device(\n",
    "        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Make sure the parser is in evaluation mode so it's not using things like dropout\n",
    "    parser.eval()\n",
    "\n",
    "    parse_sentence(sentence, parser, device, dataset)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    argparser = argparse.ArgumentParser()\n",
    "    argparser.add_argument(\n",
    "        \"--max_train_iters\",\n",
    "        help=\"Maximum training \" + \"iterations during one epoch (debug only)\",\n",
    "        type=int,\n",
    "        default=-1,\n",
    "        required=False)\n",
    "    argparser.add_argument(\n",
    "        \"--parser_name\",\n",
    "        help=\"Name used to save parser\",\n",
    "        type=str,\n",
    "        default=\"parser\",\n",
    "        required=False)\n",
    "    argparser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        required=False)\n",
    "    argparser.add_argument(\n",
    "        \"--print_every_iters\",\n",
    "        help=\"How often to print \" + \"updates during training\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        required=False)\n",
    "    argparser.add_argument(\n",
    "        \"--train\", help=\"Train the model\", action='store_true')\n",
    "    argparser.add_argument(\n",
    "        \"--test\", help=\"Test the model\", action='store_true')\n",
    "    argparser.add_argument(\n",
    "        \"--load_model_file\",\n",
    "        help=\"Load the specified \" + \"saved model for testing\",\n",
    "        type=str,\n",
    "        default=None)\n",
    "    argparser.add_argument(\n",
    "        \"--parse_sentence\",\n",
    "        help=\"Parses the example sentence using a trained parser\",\n",
    "        type=str,\n",
    "        required=False)    \n",
    "\n",
    "    args = argparser.parse_args()\n",
    "    parser = None\n",
    "    if args.train:\n",
    "        parser = train(\n",
    "            max_iters=args.max_train_iters,\n",
    "            num_epochs=args.num_epochs,\n",
    "            parser_name=args.parser_name,\n",
    "            print_every_iters=args.print_every_iters)\n",
    "    if args.test:\n",
    "        if parser is None or args.load_model_file is not None:\n",
    "            # load parser object\n",
    "            print('Loading saved parser for testing')\n",
    "            load_file = args.load_model_file\n",
    "\n",
    "            if load_file is None:\n",
    "                # Back off to see if we can keep going\n",
    "                load_file = 'saved_weights/parser-epoch-1.mdl'\n",
    "\n",
    "            print('Testing using model saved at %s' % load_file)\n",
    "            parser = torch.load(load_file)\n",
    "            device = torch.device(\n",
    "                \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "            parser.to(device)\n",
    "\n",
    "        test(parser)\n",
    "    if args.parse_sentence:\n",
    "        if parser is None or args.load_model_file is not None:\n",
    "            # load parser object\n",
    "            print('Loading saved parser for testing')\n",
    "            load_file = args.load_model_file\n",
    "\n",
    "            if load_file is None:\n",
    "                # Back off to see if we can keep going\n",
    "                load_file = 'saved_weights/parser-epoch-1.mdl'\n",
    "\n",
    "            print('Testing using model saved at %s' % load_file)\n",
    "            parser = torch.load(load_file)\n",
    "            device = torch.device(\n",
    "                \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "            parser.to(device)\n",
    "\n",
    "        parse_example(parser, args.parse_sentence)\n",
    "\n",
    "    if not (args.train or args.test or args.parse_sentence):\n",
    "        print('None of --train, --test, or --parse_sentence specified! Doing nothing...')\n",
    "        argparser.print_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model_skeleton.py\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ParserModel(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, config, word_embeddings=None, pos_embeddings=None,\n",
    "                 dep_embeddings=None):\n",
    "        self.config = config\n",
    "        # These are the hyper-parameters for choosing how many embeddings to\n",
    "        # encode in the input layer.  See the last paragraph of 3.1\n",
    "        n_w = config.word_features_types # 18\n",
    "        n_p = config.pos_features_types # 18\n",
    "        n_d = config.dep_features_types # 12\n",
    "        # Copy the Embedding data that we'll be using in the model.  Note that the\n",
    "        # model gets these in the constructor so that the embeddings can come\n",
    "        # from anywhere (the model is agnostic to the source of the embeddings).\n",
    "        self.word_embeddings = nn.Embedding(config.word_vocab_size,\n",
    "                                       config.embedding_dim)\n",
    "        self.pos_embeddings = nn.Embedding(config.pos_vocab_size,\n",
    "                                      config.embedding_dim)\n",
    "        self.dep_embeddings = nn.Embedding(config.word_vocab_size,\n",
    "                                       config.embedding_dim)\n",
    "        # Create the first layer of the network that transform the input data\n",
    "        # (consisting of embeddings of words, their corresponding POS tags, and\n",
    "        # the arc labels) to the hidden layer raw outputs.\n",
    "        self.embed_to_hidden = nn.Linear(config.embedding_dim*(n_w+n_p+n_d, config.hidden_size))\n",
    "        \n",
    "\n",
    "        # TODO\n",
    "\n",
    "        \n",
    "        # After the activation of the hidden layer, you'll be randomly zero-ing\n",
    "        # out a percentage of the activations, which is a process known as\n",
    "        # \"Dropout\".  Dropout helps the model avoid looking at the activation of\n",
    "        # one particular neuron and be more robust.  (In essence, dropout is\n",
    "        # turning the one network into an *ensemble* of networks).  Create a\n",
    "        # Dropout layer here that we'll use later in the forward() call.\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "\n",
    "        # TODO\n",
    "        # Create the output layer that maps the activation of the hidden layer to\n",
    "        # the output classes (i.e., the valid transitions)\n",
    "        self.hidden_to_logits = nn.Linear(config.hidden_size, config.n_classes)\n",
    "\n",
    "        # TODO\n",
    "\n",
    "        # Initialize the weights of both layers\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # initialize each layer's weights to be uniformly distributed within this\n",
    "        # range of +/-initrange.  This initialization ensures the weights have something to\n",
    "        # start with for computing gradient descent and generally leads to\n",
    "        # faster convergence.\n",
    "        initrange = 0.1\n",
    "        init.xavier_uniform_(self.embed_to_hidden.weight)\n",
    "        init.xavier_uniform_(self.hidden_to_logits.weight)\n",
    "\n",
    "    def lookup_embeddings(self, word_indices, pos_indices, dep_indices, keep_pos = 1):\n",
    "        \n",
    "        # Based on the IDs, look up the embeddings for each thing we need.  Note\n",
    "        # that the indices are a list of which embeddings need to be returned.\n",
    "        w_embeddings=self.word_embeddings(word_indices)\n",
    "        p_embeddings=self.pos_embeddings(pos_indices)\n",
    "        d_embeddings=self.dep_embeddings(dep_indices)\n",
    "        x = x.view(-1, self.n_features * self.embed_size) \n",
    "\n",
    "        # TODO\n",
    "        \n",
    "        return w_embeddings, p_embeddings, d_embeddings\n",
    "\n",
    "    def forward(self, word_indices, pos_indices, dep_indices):\n",
    "        \"\"\"\n",
    "        Computes the next transition step (shift, reduce-left, reduce-right)\n",
    "        based on the current state of the input.\n",
    "        \n",
    "\n",
    "        The indices here represent the words/pos/dependencies in the current\n",
    "        context, which we'll need to turn into vectors.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Look up the embeddings for this prediction.  Note that word_indices is\n",
    "        # the list of certain words currently on the stack and buffer, rather\n",
    "        # than a single word\n",
    "        w_embeddings, p_embeddings, d_embeddings = self.lookup_embeddings(word_indices,pos_indices,dep_indices)\n",
    "        # TODO\n",
    "\n",
    "        # Since we're converting lists of indices, we're getting a matrix back\n",
    "        # out (each index becomes a vector).  We need to turn these into\n",
    "        # single-dimensional vector (Flatten each of the embeddings into a\n",
    "        # single dimension).  Note that the first dimension is the batch.  For\n",
    "        # example, if we have a batch size of 2, 3 words per context, and 5\n",
    "        # dimensions per embedding, word_embeddings should be tensor with size\n",
    "        # (2,3,5).  We need it to be a tensor with size (2,15), which makes the\n",
    "        # input just like that flat input vector you see in the network diagram.\n",
    "        #\n",
    "        # HINT: you don't need to copy data here, only reshape the tensor.\n",
    "        # Functions like \"view\" (similar to numpy's \"reshape\" function will be\n",
    "        # useful here.        \n",
    "\n",
    "        # TODO\n",
    "        w_embeddings = w_embeddings.view(-1,  self.config.word_features_types * self.config.embedding_dim) \n",
    "        p_embeddings = p_embeddings.view(-1,  self.config.word_features_types * self.config.embedding_dim) \n",
    "        d_embeddings = d_embeddings.view(-1,  self.config.word_features_types * self.config.embedding_dim) \n",
    "                                         \n",
    "        \n",
    "        # Compute the raw hidden layer activations from the concatentated input\n",
    "        # embeddings.\n",
    "        #\n",
    "        # NOTE: if you're attempting the optional parts where you want to\n",
    "        # compute separate weight matrices for each type of input, you'll need\n",
    "        # do this step for each one!\n",
    "        embeddings=torch.cat(w_embeddings,p_embeddings,d_embeddings,0)                                 \n",
    "        t = self.embed_to_hidden(embeddings)\n",
    "\n",
    "        # TODO\n",
    "        \n",
    "        \n",
    "        # Compute the cubic activation function here.\n",
    "        #\n",
    "        # NOTE: Pytorch doesn't have a cubic activation function in the library\n",
    "\n",
    "        # TODO\n",
    "        t = torch.pow(t,2)\n",
    "        \n",
    "\n",
    "        # Now do dropout for final activations of the first hidden layer\n",
    "        t = self.dropout(t)\n",
    "\n",
    "        # TODO\n",
    "\n",
    "        # Multiply the activation of the first hidden layer by the weights of\n",
    "        # the second hidden layer and pass that through a ReLU non-linearity for\n",
    "        # the final output activations.\n",
    "        #\n",
    "        # NOTE 1: this output does not need to be pushed through a softmax if\n",
    "        # you're going to evaluate the output using the CrossEntropy loss\n",
    "        # function, which will compute the softmax intrinsically as a part of\n",
    "        # its optimization when computing the loss.\n",
    "\n",
    "        # TODO\n",
    "        t = self.hidden_to_logits(t)\n",
    "        output= F.relu(t)\n",
    "\n",
    "        return output    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
